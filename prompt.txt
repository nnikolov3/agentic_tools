commit 01529a8982b4f1147ce3c3dcd41f913e508f15ce
Author: Niko Nikolov <nikolay.niko.nikolov@gmail.com>
Date:   Tue Oct 28 17:45:01 2025 -0700

    feat(agent): Add ConfigurationBuilderAgent
    
    This commit introduces a new agent, `ConfigurationBuilderAgent`, which is responsible for automatically generating a `toml` configuration file for a project. The agent inspects the project to gather information about the file tree, languages, and dependencies, and then uses an LLM to generate the configuration.

diff --git a/conf/agentic_tools.toml b/conf/agentic_tools.toml
index 0940db2..5b32225 100644
--- a/conf/agentic_tools.toml
+++ b/conf/agentic_tools.toml
@@ -128,6 +128,7 @@ Your response must be a single JSON object with the following structure:
   "short_summary": "<A one or two sentence summary of the document's purpose>",
   "content": "<A detailed and comprehensive write-up of the document's key information>",
   "tags": ["<A list of relevant keywords or tags>"]
+  "practical-examples": ["A list of relevant examples"]
 }
 '''
 # The target size for text chunks before embedding.
diff --git a/main.py b/main.py
index b152302..fc9f0d4 100755
--- a/main.py
+++ b/main.py
@@ -50,6 +50,7 @@ INGEST_KNOWLEDGE_BANK: str = "ingest_knowledge_bank"
 EXPERT: str = "expert"
 KNOWLEDGE_BASE_BUILDER: str = "knowledge_base_builder"
 LINTER_ANALYST: str = "linter_analyst"
+CONFIGURATION_BUILDER: str = "configuration_builder"
 VALID_AGENTS: Tuple[str, ...] = (
     COMMENTATOR,
     DEVELOPER,
@@ -60,6 +61,7 @@ VALID_AGENTS: Tuple[str, ...] = (
     EXPERT,
     KNOWLEDGE_BASE_BUILDER,
     LINTER_ANALYST,
+    CONFIGURATION_BUILDER,
 )
 
 
@@ -327,6 +329,26 @@ async def linter_analyst_tool(
     return await _run_agent_tool(LINTER_ANALYST, chat, filepath, target_directory)
 
 
+@mcp.tool(description="Automatically generates a TOML configuration file for a project.")
+async def configuration_builder_tool(
+    chat: Optional[str] = None,
+    filepath: Optional[str | PathLike[str]] = None,
+    target_directory: Path = Path.cwd(),
+) -> Any:
+    """
+    Invokes the 'configuration_builder' agent to generate a project configuration file.
+
+    Args:
+        chat: An optional prompt to guide the configuration generation.
+        filepath: The path to the output file where the configuration will be saved.
+        target_directory: The root directory of the project to analyze.
+
+    Returns:
+        The result of the agent's operation.
+    """
+    return await _run_agent_tool(CONFIGURATION_BUILDER, chat, filepath, target_directory)
+
+
 # --- CLI Execution Logic ---
 
 
diff --git a/src/agents/agent.py b/src/agents/agent.py
index 2866e0e..afc1375 100644
--- a/src/agents/agent.py
+++ b/src/agents/agent.py
@@ -509,6 +509,127 @@ class ExpertAgent(DefaultAgent):
     pass
 
 
+class ConfigurationBuilderAgent(Agent):
+    """
+    An agent that automatically generates a 'toml' configuration file by inspecting a project.
+    """
+
+    async def run_agent(self) -> Optional[str]:
+        """
+        Orchestrates the project inspection and configuration generation.
+        """
+        logger.info("Starting configuration builder...")
+
+        # 1. Gather project information using ShellTools
+        project_tree = await self.shell_tools.get_project_tree()
+        detected_languages = await self.shell_tools.get_detected_languages()
+
+        dependency_file = "pyproject.toml"
+        try:
+            project_dependencies = await asyncio.to_thread(self.shell_tools.read_file_content, Path(dependency_file))
+        except FileNotFoundError:
+            project_dependencies = f"{dependency_file} not found."
+
+
+        # 2. Structure the context for the LLM
+        structured_context = f"""
+        Project Analysis:
+        - File Tree:
+        {project_tree}
+        - Detected Languages (by file count):
+        {detected_languages}
+        - Dependencies (from {dependency_file}):
+        {project_dependencies}
+        """
+
+        # New, detailed prompt
+        detailed_prompt = f"""
+You are an expert system configuration builder. Your task is to generate a valid `agentic_tools.toml` configuration file for a new project based on the analysis provided below.
+
+**Project Analysis:**
+```
+{structured_context}
+```
+
+**Instructions:**
+1.  Analyze the project information to determine key settings.
+2.  Use the detected languages and dependencies to configure source directories, file extensions, and linter commands.
+3.  If the primary language is Python, configure linters like `ruff`, `mypy`, and `black`. If it's another language, suggest appropriate linters (e.g., `gofmt` for Go, `eslint` for JavaScript).
+4.  Populate all sections of the TOML file. Do not leave any sections blank.
+5.  Your final output MUST be only the raw TOML configuration content, without any explanations or markdown code fences.
+
+**TOML Template to Populate:**
+
+```toml
+# Master configuration for the new project
+[new-project]
+project_name = "New Project Name"
+project_description = "A concise description of the new project."
+
+# Configure source paths based on the file tree analysis.
+source = ["src"] # Or ["lib"], ["app"], etc.
+design_docs = ["docs/DESIGN.md"]
+project_directories = ["src", "docs", "tests"] # Adjust based on analysis
+
+# Configure file extensions based on detected languages.
+include_extensions = [".py", ".md"] # e.g., [".go"], [".js", ".css"]
+exclude_files = ["__init__.py"]
+exclude_directories = [".git", "__pycache__", "venv", ".venv", "node_modules"]
+
+# Linter configuration based on detected language.
+[new-project.linters]
+# Example for Python:
+ruff = ["ruff", "check", "--no-fix", "src/"]
+mypy = ["mypy", "."]
+black = ["black", "--check", "."]
+
+# Default agent configurations. These can be customized later.
+[new-project.architect]
+model_name = "gemini-2.5-pro"
+model_provider = "google"
+api_key = "GEMINI_API_KEY_ARCHITECT"
+# ... other agent settings
+
+[new-project.developer]
+model_name = "gemini-pro-latest"
+model_provider = "google"
+api_key = "GEMINI_API_KEY_DEVELOPER"
+# ... other agent settings
+
+# Add other standard agent configurations like commentator, approver, etc.
+```
+"""
+
+        self.response = await self.tool.run_tool(
+            chat=detailed_prompt,
+            memory_context=self.memory_context,
+        )
+
+        # 4. Post-process the response (e.g., save it)
+        if self.response:
+            await self._post_process()
+            await self._store_memory()
+
+        return self.response
+
+    async def _post_process(self) -> None:
+        """
+        Writes the generated TOML configuration to a file.
+        """
+        if not self.response:
+            return
+
+        output_path = Path(self.filepath) if self.filepath else Path("generated_config.toml")
+        await asyncio.to_thread(
+            lambda: self.shell_tools.write_file(output_path, self.response)
+        )
+        logger.info("Successfully wrote configuration to '%s'.", output_path)
+
+    async def _assess_context_quality(self) -> float:
+        """Returns a default quality score of 1.0."""
+        return 1.0
+
+
 AGENT_CLASSES: dict[str, type[Agent]] = {
     "readme_writer": ReadmeWriterAgent,
     "commentator": CommentatorAgent,
@@ -518,4 +639,5 @@ AGENT_CLASSES: dict[str, type[Agent]] = {
     "expert": ExpertAgent,
     "knowledge_base_builder": KnowledgeBaseAgent,
     "linter_analyst": LinterAnalystAgent,
+    "configuration_builder": ConfigurationBuilderAgent,
 }
diff --git a/src/tools/shell_tools.py b/src/tools/shell_tools.py
index 0fd75ed..143e36a 100644
--- a/src/tools/shell_tools.py
+++ b/src/tools/shell_tools.py
@@ -377,6 +377,52 @@ class ShellTools:
 
         return "\n".join(report_parts)
 
+    async def get_project_tree(self) -> str:
+        """
+        Generates a file and directory tree.
+        """
+        try:
+            process = await asyncio.create_subprocess_exec(
+                "tree",
+                "-L",
+                "3",
+                stdout=asyncio.subprocess.PIPE,
+                stderr=asyncio.subprocess.PIPE,
+            )
+            stdout, stderr = await process.communicate()
+            output = stdout.decode(self.encoding, errors="ignore") + stderr.decode(
+                self.encoding,
+                errors="ignore",
+            )
+            return output.strip()
+        except FileNotFoundError:
+            logger.warning("'tree' command not found. Please install it.")
+            return "tree command not found"
+        except Exception as e:
+            logger.error("Error running 'tree' command: %s", e)
+            return f"Error running 'tree' command: {e}"
+
+    async def get_detected_languages(self) -> str:
+        """
+        Detects the primary programming language by analyzing file extensions.
+        """
+        try:
+            cmd = "find . -type f -name '*.*' | sed 's/.*\\.//' | sort | uniq -c | sort -nr | head -n 5"
+            process = await asyncio.create_subprocess_shell(
+                cmd,
+                stdout=asyncio.subprocess.PIPE,
+                stderr=asyncio.subprocess.PIPE,
+            )
+            stdout, stderr = await process.communicate()
+            output = stdout.decode(self.encoding, errors="ignore") + stderr.decode(
+                self.encoding,
+                errors="ignore",
+            )
+            return output.strip()
+        except Exception as e:
+            logger.error("Error detecting languages: %s", e)
+            return f"Error detecting languages: {e}"
+
     def get_git_info(self) -> Dict[str, Optional[str]]:
         git_info: Dict[str, Optional[str]] = {"username": None, "url": None}
         try:
diff --git a/src/tools/tool.py b/src/tools/tool.py
index cd87b40..43c3e00 100644
--- a/src/tools/tool.py
+++ b/src/tools/tool.py
@@ -161,6 +161,8 @@ class Tool:
                 return self._create_approver_payload(chat)
             case "linter_analyst":
                 return self._create_linter_analyst_payload(chat, memory_context)
+            case "configuration_builder":
+                return self._create_configuration_builder_payload(chat)
             case _:
                 return self._create_default_payload(chat, memory_context)
 

commit c2fed92c1f752788675331d2c9f68a6b0207478d
Author: Niko Nikolov <nikolay.niko.nikolov@gmail.com>
Date:   Tue Oct 28 17:44:48 2025 -0700

    feat(knowledge_base): Enhance ingestion with summaries and metadata
    
    This commit introduces several improvements to the knowledge base ingestion process:
    
    - A separate Qdrant collection is created for LLM-generated summaries, providing a dedicated space for high-level document overviews.
    - The ingestion script now captures additional metadata for each chunk, including file extension, ingestion date, and modification date.
    - A warning is prepended to the content of old files to prevent them from providing stale context to the agents.

diff --git a/src/scripts/ingest_knowledge_bank.py b/src/scripts/ingest_knowledge_bank.py
index 8fd994f..3bddcb7 100644
--- a/src/scripts/ingest_knowledge_bank.py
+++ b/src/scripts/ingest_knowledge_bank.py
@@ -25,6 +25,7 @@ import uuid
 from collections import Counter
 from collections.abc import Awaitable, Callable
 from dataclasses import dataclass, field
+from datetime import datetime, timezone, timedelta
 from pathlib import Path
 from typing import Any, Final, Optional
 
@@ -88,6 +89,7 @@ class IngestionConfig:
     chunk_overlap: int
     qdrant_batch_size: int
     concurrency_limit: int
+    old_file_threshold_days: int
 
 
 @dataclass(frozen=True)
@@ -95,6 +97,7 @@ class MemoryConfig:
     """Configuration settings for the memory/vector database."""
 
     collection_name: str
+    summaries_collection_name: str
     embedding_model_name: str
     device: str
     qdrant_config: dict[str, Any] = field(default_factory=dict)
@@ -254,6 +257,7 @@ class KnowledgeBankIngestor:
             chunk_overlap=config.get("chunk_overlap", 200),
             qdrant_batch_size=config.get("qdrant_batch_size", 128),
             concurrency_limit=config.get("concurrency_limit", 5),
+            old_file_threshold_days=config.get("old_file_threshold_days", 730),
         )
 
     @staticmethod
@@ -261,6 +265,7 @@ class KnowledgeBankIngestor:
         """Loads and validates the memory-specific configuration."""
         return MemoryConfig(
             collection_name=config.get("knowledge_bank", "knowledge-bank"),
+            summaries_collection_name=config.get("summaries_collection_name", "knowledge-bank-summaries"),
             embedding_model_name=config.get(
                 "embedding_model", "mixedbread-ai/mxbai-embed-large-v1"
             ),
@@ -281,15 +286,19 @@ class KnowledgeBankIngestor:
             raise RuntimeError("Embedding generation failed") from error
 
     async def _ensure_collection_exists(self) -> None:
-        """Creates the Qdrant collection and payload indexes if they don't exist."""
-        await self.qdrant_manager.ensure_collection_exists(
-            collection_name=self.memory_config.collection_name,
-            embedding_size=self.embedding_size,
-            payload_indexes=[
-                (PROCESSED_CONTENT_HASH_FIELD, "keyword"),
-                (RAW_FILE_HASH_FIELD, "keyword"),
-            ],
-        )
+        """Creates the Qdrant collections and payload indexes if they don't exist."""
+        for collection_name in [
+            self.memory_config.collection_name,
+            self.memory_config.summaries_collection_name,
+        ]:
+            await self.qdrant_manager.ensure_collection_exists(
+                collection_name=collection_name,
+                embedding_size=self.embedding_size,
+                payload_indexes=[
+                    (PROCESSED_CONTENT_HASH_FIELD, "keyword"),
+                    (RAW_FILE_HASH_FIELD, "keyword"),
+                ],
+            )
 
     async def _resolve_kb_vector_names(self) -> None:
         """Inspects the live Qdrant collection to determine its vector names."""
@@ -394,6 +403,30 @@ class KnowledgeBankIngestor:
                 f"Qdrant upsert status not OK for '{file_path}': {status}"
             )
 
+    async def _upsert_summary(self, summary: str, file_path: Path) -> None:
+        """Upserts a summary to the summaries collection."""
+        if not summary:
+            return
+
+        embedding = self._generate_embeddings([summary])[0]
+        point = models.PointStruct(
+            id=str(uuid.uuid4()),
+            vector={self.kb_dense_name: embedding} if self.kb_dense_name else embedding,
+            payload={
+                TEXT_CONTENT_FIELD: summary,
+                ORIGINAL_FILE_PATH_FIELD: str(file_path),
+                "ingestion_date": datetime.now(timezone.utc).isoformat(),
+                "modification_date": datetime.fromtimestamp(
+                    file_path.stat().st_mtime, tz=timezone.utc
+                ).isoformat(),
+            },
+        )
+        await self.qdrant_client.upsert(
+            collection_name=self.memory_config.summaries_collection_name,
+            points=[point],
+            wait=True,
+        )
+
     def _create_points_from_chunks(
         self,
         chunks: list[str],
@@ -414,6 +447,11 @@ class KnowledgeBankIngestor:
                 RAW_FILE_HASH_FIELD: raw_file_hash,
                 PROCESSED_CONTENT_HASH_FIELD: chunk_hash,
                 CHUNK_ID_FIELD: index,
+                "file_extension": file_path.suffix,
+                "ingestion_date": datetime.now(timezone.utc).isoformat(),
+                "modification_date": datetime.fromtimestamp(
+                    file_path.stat().st_mtime, tz=timezone.utc
+                ).isoformat(),
             }
 
             vector_payload: models.VectorStruct = (
@@ -464,6 +502,7 @@ class KnowledgeBankIngestor:
 
         llm_summary = await self._get_llm_summary_for_pdf(file_path)
         cleaned_summary = self.clean_text(llm_summary)
+        await self._upsert_summary(cleaned_summary, file_path)
         return f"{cleaned_summary}\n\n{cleaned_text}"
 
     async def _process_json_content(self, file_path: Path) -> str:
@@ -483,6 +522,7 @@ class KnowledgeBankIngestor:
                 cleaned_content, file_path
             )
             cleaned_summary = self.clean_text(llm_summary)
+            await self._upsert_summary(cleaned_summary, file_path)
             return f"{cleaned_summary}\n\n{cleaned_content}"
         except (OSError, json.JSONDecodeError) as error:
             logger.exception("Error processing JSON file %s.", file_path)
@@ -526,12 +566,25 @@ class KnowledgeBankIngestor:
         async with self.semaphore:
             logger.info("Processing file: %s", file_path)
 
+            # Check if the file is old
+            modification_time = datetime.fromtimestamp(file_path.stat().st_mtime, tz=timezone.utc)
+            age = datetime.now(timezone.utc) - modification_time
+            if age > timedelta(days=self.ingestion_config.old_file_threshold_days):
+                warning = (
+                    f"Warning: This document is from {modification_time.year} and may be outdated. "
+                    "Use for informational and research purposes only.\n\n"
+                )
+            else:
+                warning = ""
+
             # 1. Extract and clean content based on file type.
             processed_content = await self._extract_content_from_file(file_path)
             if not processed_content:
                 logger.warning("No content extracted from: %s", file_path)
                 return "skipped"
 
+            processed_content = warning + processed_content
+
             # 2. Hash content to check for duplicates.
             raw_file_hash = hashlib.sha256(
                 processed_content.encode("utf-8")

commit 08dcc553a916aa987d04eca58863e5f1df92732e
Author: Niko Nikolov <nikolay.niko.nikolov@gmail.com>
Date:   Tue Oct 28 16:55:21 2025 -0700

    removing explanation

diff --git a/explanation.txt b/explanation.txt
deleted file mode 100644
index cedf37a..0000000
--- a/explanation.txt
+++ /dev/null
@@ -1,15 +0,0 @@
-1. Improved Data Cleaning:
-    *   **What:** A new `clean_text` function has been implemented and applied to all extracted content (PDFs, JSON, Markdown).
-    *   **Why:** To correct Optical Character Recognition (OCR) errors, such as ligatures (e.g., `con\ufb01gured` is now handled) and remove non-printable characters, ensuring cleaner text data.
-
-2.  **Enhanced Chunking Strategy:**
-    *   **What:** The `RecursiveCharacterTextSplitter` from `langchain_text_splitters` is now used more consistently for semantic chunking across all document types. For Markdown, it's applied after `MarkdownHeaderTextSplitter` to leverage header structures.
-    *   **Why:** To create more contextually relevant text chunks by splitting documents based on semantic structure rather than just fixed character counts, improving retrieval coherence.
-
-3.  **Richer Metadata:**
-    *   **What:** For Markdown files, section header information is now extracted and stored as metadata alongside the text chunks in Qdrant. For other file types (PDF, JSON), basic metadata like source file path and chunk ID is included.
-    *   **Why:** To provide the RAG system with more context during retrieval, allowing it to return more precise and relevant information by understanding the source section of a retrieved chunk.
-
-4.  **LLM Summarization:**
-    *   **What:** LLM summaries are generated for PDF and JSON documents. These summaries are cleaned, prepended to the main content, and then processed along with the rest of the text.
-    *   **Why:** To provide a concise overview of the document's content, which can be beneficial for retrieval and understanding, especially for longer documents.
\ No newline at end of file
diff --git a/pyproject.toml b/pyproject.toml
index 1eb33f8..3772871 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -90,4 +90,4 @@ bad_imports = ["pickle", "telnetlib"]
 bad_name_sets = [
   # e.g., use yaml.safe_load instead of yaml.load
   {qualnames = ["yaml.load"], message = "Use yaml.safe_load instead of yaml.load"},
-]
+]
\ No newline at end of file
